from langchain_community.document_loaders import ObsidianLoader
from langchain.text_splitter import CharacterTextSplitter
import hashlib
import json
import os
import requests
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()

# Initialize SID API details
SID_API_KEY = os.getenv("SID_API_KEY")
CAPSULE_ID = os.getenv("SID_CAPSULE_ID")
SID_BASE_URL = f"https://{CAPSULE_ID}.sid.ai/data"

# Initialize Obsidian loader
obsidian_vault_path = os.getenv("OBSIDIAN_PATH")
obsidian_loader = ObsidianLoader(obsidian_vault_path)

# Function to calculate file hash
def calculate_file_hash(file_path):
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5()
        chunk = f.read(8192)
        while chunk:
            file_hash.update(chunk)
            chunk = f.read(8192)
    return file_hash.hexdigest()

# Load or create the file hash database
hash_db_path = os.path.join(obsidian_vault_path, ".sid_hash_db.json")
if os.path.exists(hash_db_path):
    with open(hash_db_path, "r") as f:
        file_hash_db = json.load(f)
else:
    file_hash_db = {}

# Load documents from Obsidian vault
documents = []
for root, _, files in os.walk(obsidian_vault_path):
    for file in files:
        if file.endswith(".md"):
            file_path = os.path.join(root, file)
            current_hash = calculate_file_hash(file_path)
            
            if file_path not in file_hash_db or file_hash_db[file_path] != current_hash:
                # File is new or modified
                documents.extend(obsidian_loader.load())
                file_hash_db[file_path] = current_hash

# Save updated file hash database
with open(hash_db_path, "w") as f:
    json.dump(file_hash_db, f)

# Split documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = text_splitter.split_documents(documents)

# Function to add document to SID capsule
def add_to_sid(content, metadata):
    url = f"{SID_BASE_URL}/file"
    headers = {
        "Authorization": f"Bearer {SID_API_KEY}",
        "Content-Type": "multipart/form-data"
    }
    data = {
        "file": content,
        "metadata": json.dumps(metadata),
        "time_authored": datetime.now().isoformat() + "Z",
        "uri": metadata.get("source", "")
    }
    response = requests.post(url, headers=headers, files=data)
    return response.status_code == 200

# Add documents to SID capsule
successful_uploads = 0
for i, doc in enumerate(split_docs, 1):
    if add_to_sid(doc.page_content, doc.metadata):
        successful_uploads += 1
        print(f"Uploaded document chunk {i}/{len(split_docs)} to SID capsule")
    else:
        print(f"Failed to upload document chunk {i}/{len(split_docs)} to SID capsule")

print(f"Added {successful_uploads} out of {len(split_docs)} document chunks to SID capsule")