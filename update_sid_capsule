from langchain_community.document_loaders import ObsidianLoader
from langchain.text_splitter import CharacterTextSplitter
import json
import os
import requests
import hashlib
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path
from google.cloud import logging_v2
from google.auth import default

load_dotenv()

# Initialize constants
SID_API_KEY = os.getenv("SID_API_KEY")
CAPSULE_ID = os.getenv("SID_CAPSULE_ID")
SID_BASE_URL = f"https://{CAPSULE_ID}.sid.ai/data"
PROJECT_ID = os.getenv("GCP_PROJECT_ID")
OBSIDIAN_PATH = os.getenv("OBSIDIAN_PATH")

# Set up GCP Cloud Logging
credentials, project = default()
client = logging_v2.Client(project=PROJECT_ID, credentials=credentials)
logger = client.logger('sid-capsule-update-logs')

# Get the directory of the current script
script_dir = Path(__file__).parent.absolute()
hash_db_path = script_dir / ".sid_hash_db.json"

def calculate_file_hash(file_path):
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5()
        chunk = f.read(8192)
        while chunk:
            file_hash.update(chunk)
            chunk = f.read(8192)
    return file_hash.hexdigest()

def load_hash_database():
    if hash_db_path.exists():
        with hash_db_path.open("r") as f:
            return json.load(f)
    return {}

def save_hash_database(file_hash_db):
    with hash_db_path.open("w") as f:
        json.dump(file_hash_db, f)

def load_documents():
    obsidian_loader = ObsidianLoader(OBSIDIAN_PATH)
    file_hash_db = load_hash_database()
    new_or_modified_docs = []
    try:
        all_docs = obsidian_loader.load()
        for doc in all_docs:
            file_path = doc.metadata.get('path')
            if file_path:
                current_hash = calculate_file_hash(file_path)
                if file_path not in file_hash_db or file_hash_db[file_path] != current_hash:
                    new_or_modified_docs.append(doc)
                    file_hash_db[file_path] = current_hash
        save_hash_database(file_hash_db)
        logger.log_text(f"Loaded {len(new_or_modified_docs)} new or modified documents out of {len(all_docs)} total", severity="INFO")
        return new_or_modified_docs
    except Exception as e:
        logger.log_text(f"Error loading documents from Obsidian vault: {str(e)}", severity="ERROR")
        return []

def split_documents(documents):
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    logger.log_text(f"Total document chunks after splitting: {len(split_docs)}", severity="INFO")
    if split_docs:
        logger.log_text(f"First chunk content (truncated): {split_docs[0].page_content[:100]}...", severity="INFO")
    else:
        logger.log_text("No document chunks created", severity="WARNING")
    return split_docs

def add_to_sid(content, metadata):
    url = f"{SID_BASE_URL}/file"
    headers = {
        "Authorization": f"Bearer {SID_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "file": content,
        "metadata": metadata,
        "time_authored": datetime.now().isoformat() + "Z",
        "uri": metadata.get("source", "")
    }
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        logger.log_text(f"Error uploading to SID: {str(e)}", severity="ERROR")
        return False

def upload_documents(split_docs):
    successful_uploads = 0
    for i, doc in enumerate(split_docs, 1):
        if add_to_sid(doc.page_content, doc.metadata):
            successful_uploads += 1
            logger.log_text(f"Uploaded document chunk {i}/{len(split_docs)} to SID capsule", severity="INFO")
        else:
            logger.log_text(f"Failed to upload document chunk {i}/{len(split_docs)} to SID capsule", severity="ERROR")
    return successful_uploads

def print_summary(total_chunks, successful_uploads, failed_uploads, duration):
    print("\n--- SID Capsule Update Summary ---")
    print(f"Total document chunks processed: {total_chunks}")
    print(f"Successful uploads: {successful_uploads}")
    print(f"Failed uploads: {failed_uploads}")
    print(f"Duration: {duration}")
    print("Logs available in GCP Cloud Logging")
    print("----------------------------------")

def main():
    print(f"Using project: {PROJECT_ID}")
    
    try:
        logger.log_text("Starting SID capsule update process", severity="INFO")
        start_time = datetime.now()

        new_or_modified_docs = load_documents()
        split_docs = split_documents(new_or_modified_docs)
        successful_uploads = upload_documents(split_docs)

        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.log_text(f"SID capsule update process completed in {duration}", severity="INFO")
        logger.log_text(f"Total document chunks processed: {len(split_docs)}", severity="INFO")
        logger.log_text(f"Successful uploads: {successful_uploads}", severity="INFO")
        logger.log_text(f"Failed uploads: {len(split_docs) - successful_uploads}", severity="INFO")

        print_summary(len(split_docs), successful_uploads, len(split_docs) - successful_uploads, duration)

    except Exception as e:
        logger.log_text(f"An error occurred: {str(e)}", severity="ERROR")
        with open('error_log.txt', 'a') as f:
            f.write(f"{datetime.now()}: {str(e)}\n")

if __name__ == "__main__":
    main()