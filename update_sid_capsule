from langchain_community.document_loaders import ObsidianLoader
from langchain.text_splitter import CharacterTextSplitter
import hashlib
import json
import os
import requests
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path

load_dotenv()

# Initialize SID API details
SID_API_KEY = os.getenv("SID_API_KEY")
CAPSULE_ID = os.getenv("SID_CAPSULE_ID")
SID_BASE_URL = f"https://{CAPSULE_ID}.sid.ai/data"

# Initialize Obsidian loader
obsidian_vault_path = os.getenv("OBSIDIAN_PATH")
obsidian_loader = ObsidianLoader(obsidian_vault_path)

# Function to calculate file hash
def calculate_file_hash(file_path):
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5()
        chunk = f.read(8192)
        while chunk:
            file_hash.update(chunk)
            chunk = f.read(8192)
    return file_hash.hexdigest()

# Get the directory of the current script
script_dir = Path(__file__).parent.absolute()

# Define the path for the hash database in the mirror directory
hash_db_path = script_dir / ".sid_hash_db.json"

# Load or create the file hash database
if hash_db_path.exists():
    with hash_db_path.open("r") as f:
        file_hash_db = json.load(f)
    print(f"Loaded existing hash database with {len(file_hash_db)} entries")
else:
    file_hash_db = {}
    print("Created new hash database")

# Load documents from Obsidian vault
documents = []
files_processed = 0
files_loaded = 0
files_skipped = 0
force_reload_count = 0  # New counter for forced reloads

for root, _, files in os.walk(obsidian_vault_path):
    for file in files:
        if file.endswith(".md"):
            files_processed += 1
            file_path = os.path.join(root, file)
            current_hash = calculate_file_hash(file_path)
            
            # Force reload of all files for now
            try:
                file_loader = ObsidianLoader(file_path)
                docs = file_loader.load()
                print(f"Loaded file: {file_path}")
                print(f"  Number of documents: {len(docs)}")
                if docs:
                    print(f"  First document content (truncated): {docs[0].page_content[:100]}...")
                else:
                    print("  No documents loaded from this file")
                documents.extend(docs)
                file_hash_db[file_path] = current_hash
                files_loaded += 1
                force_reload_count += 1
            except Exception as e:
                print(f"Error loading file {file_path}: {str(e)}")

print(f"Processed {files_processed} files, loaded {files_loaded} files (including {force_reload_count} forced reloads), skipped {files_skipped} files")
print(f"Total documents loaded: {len(documents)}")

# After processing all files, save the updated hash database
with hash_db_path.open("w") as f:
    json.dump(file_hash_db, f)
print(f"Saved hash database with {len(file_hash_db)} entries to {hash_db_path}")

# Split documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = text_splitter.split_documents(documents)
print(f"Total document chunks after splitting: {len(split_docs)}")
if split_docs:
    print(f"First chunk content (truncated): {split_docs[0].page_content[:100]}...")
else:
    print("No document chunks created")

# Function to add document to SID capsule
def add_to_sid(content, metadata):
    url = f"{SID_BASE_URL}/file"
    headers = {
        "Authorization": f"Bearer {SID_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "file": content,
        "metadata": metadata,
        "time_authored": datetime.now().isoformat() + "Z",
        "uri": metadata.get("source", "")
    }
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error uploading to SID: {str(e)}")
        return False

# Add documents to SID capsule
successful_uploads = 0
for i, doc in enumerate(split_docs, 1):
    if add_to_sid(doc.page_content, doc.metadata):
        successful_uploads += 1
        print(f"Uploaded document chunk {i}/{len(split_docs)} to SID capsule")
    else:
        print(f"Failed to upload document chunk {i}/{len(split_docs)} to SID capsule")

print(f"Added {successful_uploads} out of {len(split_docs)} document chunks to SID capsule")